{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrabing with The New York Social Graph Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [New York Social Diary](http://www.newyorksocialdiary.com/)  \n",
    "#The data forms a natural social graph for New York's social elite.  #\n",
    "#Link = [run-of-the-mill holiday party](http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers).\n",
    "\n",
    "# In this project, we will assemble the social graph from photo captions for parties dated December 1, 2014, and before.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "# We will attack the project in three stages:\n",
    "# 1. Get a list of all the photo pages to be analyzed.\n",
    "# 2. Parse all of the captions on a sample page.\n",
    "# 3. Parse all of the captions on all pages, and assemble the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage_1 (Getting a list of all the photo pages to be analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to crawl the data.  \n",
    "#We want photos from parties on or before December 1st, 2014.  \n",
    "# Link = [Party Pictures Archive](http://www.newyorksocialdiary.com/party-pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import dill\n",
    "import nltk\n",
    "from requests_futures.sessions import FuturesSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('http://www.newyorksocialdiary.com/party-pictures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using BeautifulSoup's select or find_all methods to get the elements\n",
    "parent = soup.find('div', attrs={'class':'view-content'})\n",
    "links = soup.find_all('div', attrs={'class':'views-row'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 per page\n",
    "assert len(links) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the URL of the link, as well as the date using `datetime.strptime`.  \n",
    "link = links[0]\n",
    "date=link.find('span', attrs={'class': 'views-field views-field-created'})\n",
    "print(link.a['href'])\n",
    "print(date.text)\n",
    "# Check that the title and date match what you see visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning the URL and date parsed\n",
    "def cleandate(date):\n",
    "    import time\n",
    "    date2=''.join(date.strip().split(',')[1:3])\n",
    "    date2=time.strftime('%Y, %m, %d', time.strptime(date2, ' %B %d %Y'))\n",
    "    return date2\n",
    "\n",
    "def get_link_date(el):\n",
    "    url=el.a['href']\n",
    "    date=el.find('span', attrs={'class': 'views-field views-field-created'}).text\n",
    "    date=cleandate(date)\n",
    "    return url, date\n",
    "url, date = get_link_date(links[0])\n",
    "print(url, date)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing a function to parse all of the links on a page\n",
    "def get_links(page):\n",
    "    soup = BeautifulSoup(page.text, \"lxml\")\n",
    "    parent = soup.find('div', attrs={'class':'view-content'})\n",
    "    links = parent.find_all('div', attrs={'class': 'views-row'})\n",
    "    result =[]\n",
    "    for link in links:\n",
    "        result.append(get_link_date(link))\n",
    "    return result # A list of URL, date pairs\n",
    "results=get_links(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get 50 pairs\n",
    "assert len(get_links(page)) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing a function to filter the list of dates to those at or before a cutoff\n",
    "def filter_by_date(results, cutoff=datetime(2014, 12, 1)):\n",
    "    import time\n",
    "    cut_data=[]\n",
    "    end_time=time.strptime('2014, 12, 1', '%Y, %m, %d')\n",
    "    for result in results:\n",
    "        cur_date=result[1]\n",
    "        cur_date=time.strptime(cur_date, '%Y, %m, %d')\n",
    "        if cur_date<=end_time:\n",
    "            cut_data.append(result)\n",
    "    return cut_data\n",
    "    # Return only the elements with date <= cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_data=filter_by_date(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting the cutoff date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(filter_by_date(get_links(page))) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page1 = requests.get('http://www.newyorksocialdiary.com/party-pictures', params={'page': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=get_links(page1)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all of the party URLs\n",
    "from requests_futures.sessions import FuturesSession\n",
    "def get_page_args(i):\n",
    "    return {'url': 'http://www.newyorksocialdiary.com/party-pictures', 'params': {'page': i}}\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "futures = [session.get(**get_page_args(i)) for i in range(32)]\n",
    "all_links=[] #to get all_links without the 2014 date filtering #len of all_links: 1580\n",
    "for future in futures:\n",
    "    all_links.extend(get_links(future.result()))\n",
    "\n",
    "link_list=[]\n",
    "for future in futures:\n",
    "    link_list.extend(filter_by_date(get_links(future.result())))\n",
    "# You can use link_list.extend(others) to add the elements of others\n",
    "# to link_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_2014date(results, cutoff=datetime(2014, 12, 1)):\n",
    "    import time\n",
    "    cut_data=[]\n",
    "    end_time=time.strptime('2014, 12, 1', '%Y, %m, %d')\n",
    "    for result in results:\n",
    "        cur_date=result[1]\n",
    "        cur_date=time.strptime(cur_date, '%Y, %m, %d')\n",
    "        if cur_date>end_time:\n",
    "            cut_data.append(result)\n",
    "    return cut_data\n",
    "link_list2014=[]\n",
    "for future in futures:\n",
    "    link_list2014.extend(filter_by_2014date(get_links(future.result()))) #387 links after 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(link_list2014, open('link_list_after2014.pkd','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total of 1193 parties\n",
    "assert len(link_list) == 1193"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case,  saving the information to a file using `dill`\n",
    "dill.dump(link_list, open('nysd-links.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the list from the file to restore\n",
    "link_list = dill.load(open('nysd-links.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the number of party pages for the 95 months\n",
    "link_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dates=time.strftime('%b-%Y', time.strptime(link_list[1][1], '%Y, %m, %d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_party_counts(link_list):\n",
    "    count={}\n",
    "    for link in link_list:\n",
    "        dates=time.strftime('%b-%Y', time.strptime(link[1], '%Y, %m, %d'))\n",
    "        count[dates]=count.get(dates, 0)+1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=get_month_party_counts(link_list)\n",
    "result=list(count.items())\n",
    "def histogram():\n",
    "    return result  # Replace with the correct list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage_2 (Parsing all of the captions on a sample page.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the names out of captions for a given page. \n",
    "#[Link 1](http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) \n",
    "#[Link 2](http://www.lenoxhill.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_response=requests.get('http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood')\n",
    "captions_parent = BeautifulSoup(cap_response.text, 'lxml')\n",
    "captions=captions_parent.find_all('div', attrs={'class': 'photocaption'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are about 110\n",
    "assert abs(len(captions) - 110) < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As with the links pages, to avoid downloading a given page, \n",
    "#the next time we need to run the notebook.  \n",
    "#While we could save the files by hand, as we did before, a checkpoint library \n",
    "#like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) \n",
    "#can handle this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "def clean(caption):\n",
    "    caption = re.sub('\\\\t+|\\\\n+', '', str(caption.text).strip())\n",
    "    caption = re.sub(r'Photograph.*', ' ', caption)\n",
    "    caption = caption.strip()\n",
    "    return caption\n",
    "\n",
    "def get_captions(path):\n",
    "    captions=[]\n",
    "    response=requests.get('http://www.newyorksocialdiary.com{}'.format(path))\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    captions.extend(list(set([clean(caption) for caption in soup.select('.photocaption')])))\n",
    "    if len(captions)==0:  \n",
    "        captions.extend(list(set([clean(caption) for caption in soup.select('.field__items td font')])))\n",
    "    if len(captions)==0: \n",
    "        captions.extend(list(set([clean(caption) for caption in soup.select('div.collageformatter-collage-image__caption_content')])))\n",
    "    if len(captions)==0:\n",
    "        print(path)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions=get_captions('/party-pictures/2015/celebrating-the-neighborhood')\n",
    "print([caption for caption in captions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the same captions\n",
    "assert captions == get_captions(\"/party-pictures/2015/celebrating-the-neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are sample captions, let's start parsing names out of those captions\n",
    "import spacy\n",
    "import re\n",
    "nlp=spacy.load('en')\n",
    "def get_names_from_captions(captions):   \n",
    "    all_names=[] #names in each caption is saved as a list element in all_names\n",
    "    i=0\n",
    "    for caption in captions:\n",
    "        names=[]\n",
    "        i+=1\n",
    "        if i%500==0:\n",
    "            print(caption)\n",
    "            print('{} captions have been processed by spacy'.format(i))\n",
    "        caption = re.sub('â€™s|\\'s|Jr\\.', '', str(caption).strip())\n",
    "        caption = re.sub('\\&', 'and', caption)\n",
    "        caption = re.sub('\\\\t+|\\\\n+| +', ' ', caption)\n",
    "        caption = re.sub(r'Photograph*', '', caption)\n",
    "        caption = re.sub(r'Mrs.', 'Amyy', caption)\n",
    "        caption =caption.strip()\n",
    "        if len(caption)==0:\n",
    "            continue\n",
    "        doc=nlp(caption)\n",
    "        for obj in doc.ents:\n",
    "            if obj.label_=='PERSON':\n",
    "                name= re.sub(r'[A-Z]\\.', '', str(obj).strip())\n",
    "                name= re.sub(' +', ' ', name)\n",
    "                name=name.strip()\n",
    "                names.append(name)\n",
    "        for idx in range(len(names)-1):\n",
    "            if len(names[idx].split(' '))==1:\n",
    "                names[idx]=names[idx]+' '+names[idx+1].split(' ')[-1]\n",
    "        if len(names)!=0:\n",
    "            all_names.append(names)\n",
    "    return all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names=get_names_from_captions(captions)\n",
    "print(len(captions))\n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_names\n",
    "name_res=[]\n",
    "for names in all_names:\n",
    "    name_res.extend(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing all of the captions and extracting all the names mentioned. \n",
    "#Sorting them alphabetically, by first name, and \n",
    "#returning the first 100\n",
    "def sample_names():\n",
    "    return sorted(list(set(name_res)))[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage_3 (Parsing all of the captions on all pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running caption scraper and parser for all pages, \n",
    "#and all of the pages could take 10 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "all_captions_revised = []\n",
    "for link in link_list:\n",
    "    if i%50==0:\n",
    "        print('{} links of captions have been downloaded...'.format(i))\n",
    "    all_captions_revised.extend(get_captions(link[0]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_captions_revised)) #101713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [str(caption) for caption in all_captions_revised[190:200]]\n",
    "print(text)\n",
    "print(len(all_captions_revised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_name_edges(all_names):\n",
    "    edges=[]\n",
    "    for names in all_names:\n",
    "        edges.extend(list(itertools.combinations(names, 2))) #two-pair edge generation for the names in each caption\n",
    "    return edges #lists of lists of edge tuples, each list element from one caption [[(p1, p2), (p1, p3), (p2,p3)], [(p4, p5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_links_names = get_names_from_captions(all_captions_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(links_names, open('all_link_names_2014spacy_complete_{}.dpk'.format(i), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_names=dill.load(open('all_link_names_2014spacy_complete.dpk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_names=dill.load(open('all_link_names_2014spacy_complete.dpk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links_names) #92982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop2={}\n",
    "for i in all_links_names:\n",
    "    for name in i:\n",
    "        pop2[name]=pop2.get(name, 0)+len(i)-1\n",
    "print(len(pop2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_name_edges(all_names):\n",
    "    edges=[]\n",
    "    for names in all_names:\n",
    "        if len(names)<2:\n",
    "            continue\n",
    "        edges.extend(list(itertools.combinations(sorted(names), 2))) #two-pair edge generation for the names in each caption\n",
    "    return edges #lists of lists of edge tuples, each list element from one caption [[(p1, p2), (p1, p3), (p2,p3)], [(p4, p5)]]\n",
    "\n",
    "all_links_edges=get_name_edges(all_links_names) #197337\n",
    "print(len(all_links_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions_revised[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_names[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_edges[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: ('Susan Johnson' in x) and ('Henry Johnson' in x), all_links_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: 'Susan Johnson' in x[0], counts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts=Counter(all_links_edges)\n",
    "weighted_edges=[(i[0][0],i[0][1],i[1]) for i in counts.items()]\n",
    "print(len(weighted_edges))\n",
    "ques5=sorted(counts.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques5[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop={}\n",
    "for i in weighted_edges:\n",
    "    pop[i[0]]=pop.get(i[0], 0)+i[2]\n",
    "    pop[i[1]]=pop.get(i[1], 0)+i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sorted(pop.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(test, columns=['name','popcount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=sorted(pop2.items(), key=lambda x: x[1], reverse=True)[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(test2,columns=['name','popcount'])\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(test2, open('test2.dpk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(test2, open('test2.dpk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(weighted_edges)\n",
    "rank_res=nx.pagerank(G) #dictionary of ranking of each node\n",
    "res=sorted(rank_res.items(), key=lambda x: x[1], reverse=True)[:100] #question 4 result\n",
    "df3=pd.DataFrame(res,columns=['name','rank'])\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyzing the social graph of the New York social elite, using python's networkx library.\n",
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "def degree():\n",
    "    return test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_friends():\n",
    "    return sorted(ques6, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "grader.score(question_name='graph__best_friends', func=best_friends)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
